import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os
import urllib.request
import re
from datetime import datetime, timedelta
import io
import requests  # æ–°å¢ï¼šç”¨æ–¼ REST API å…¼å®¹æ¨¡å¼
import json      # æ–°å¢ï¼šç”¨æ–¼è™•ç† API å›å‚³æ ¼å¼

# --- æ ¸å¿ƒä¿®æ­£ï¼šå®‰å…¨å¼•å…¥å¥—ä»¶ä»¥é˜²æ­¢ App é–ƒé€€ ---
try:
    import google.generativeai as genai
    HAS_GENAI = True
except ModuleNotFoundError:
    HAS_GENAI = False

# æª¢æŸ¥ xlsxwriter æ˜¯å¦å­˜åœ¨ (Excel åŒ¯å‡ºéœ€è¦)
try:
    import xlsxwriter
    HAS_XLSXWRITER = True
except ModuleNotFoundError:
    HAS_XLSXWRITER = False
# -------------------------------------------

# ==========================================
# 0. å…¨åŸŸè¨­å®šï¼šAI é¡§å•æŒ‡ä»¤
# ==========================================
AI_CONSULTANT_PROMPT = """
# âš ï¸ SYSTEM OVERRIDE: DATA LOADING INSTRUCTION
**To the AI Analysis Model:**
The user has provided summary data tables from an advertising account.
1. **Data Source**: Summarized metrics from P1D (Yesterday), P7D (Last 7 Days), and comparison vs PP7D (Previous Period).
2. **Key Comparisons**: 
   - **P1D vs P7D**: Immediate daily alerts.
   - **P7D vs PP7D**: Week-over-Week trend analysis.

# Role
ä½ æ˜¯ä¸€ä½è³‡æ·±æˆæ•ˆå»£å‘Šåˆ†æå¸«ã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ã€‚

# Analysis Requirements
## 1. ğŸš¨ P1D ç·Šæ€¥ç•°å¸¸ (Daily Alert)
- æª¢æŸ¥ **P1D (æ˜¨æ—¥)** ç›¸è¼ƒæ–¼ **P7D (å‡å€¼)** æ˜¯å¦æœ‰ CPA æš´æ¼² (>30%) æˆ– CTR é©Ÿé™ (>20%)ã€‚
- é€™æ˜¯ã€Œæ•‘ç«ã€å±¤ç´šï¼Œè«‹å„ªå…ˆæŒ‡å‡ºéœ€è¦ç«‹å³é—œé–‰æˆ–æª¢æŸ¥çš„å»£å‘Šã€‚

## 2. ğŸ“‰ P7D vs PP7D é€±ç’°æ¯”åˆ†æ (WoW Trend)
- å°æ¯” **P7D (æœ¬é€±)** èˆ‡ **PP7D (ä¸Šé€±)**ã€‚
- æ‰¾å‡º CPA è®Šé«˜ã€CVR è®Šä½çš„ã€Œè¡°é€€è¡ŒéŠ·æ´»å‹•ã€ã€‚
- è‹¥æœ¬é€±èŠ±è²»å¢åŠ ä½† ROAS/CPA è®Šå·®ï¼Œè«‹æ¨™è¨˜ç‚ºã€Œæ“´é‡å¤±æ•— (Inefficient Scaling)ã€ã€‚
- è‹¥æœ¬é€± CTR æå‡ä½† CVR ä¸‹é™ï¼Œè«‹æ¨™è¨˜ç‚ºã€Œæµé‡å“è³ªè®Šå·® (Traffic Quality Drop)ã€ã€‚

## 3. ç¶œåˆå„ªåŒ–å»ºè­°
- é‡å°è¡°é€€é …ç›®æå‡ºå…·é«”å‡è¨­ï¼ˆç´ æç–²ä¹ï¼Ÿç«¶åƒ¹æ¿€çƒˆï¼Ÿå—çœ¾é£½å’Œï¼Ÿï¼‰ã€‚
- è«‹æ¢åˆ—å¼çµ¦å‡ºå…·é«”çš„èª¿æ•´å»ºè­°ï¼ˆä¾‹å¦‚ï¼šæš«åœå»£å‘Šã€æ›´æ›å—çœ¾ã€å„ªåŒ–è½åœ°é ï¼‰ã€‚
"""

# ==========================================
# 1. åŸºç¤è¨­å®šèˆ‡å­—å‹è™•ç†
# ==========================================
st.set_page_config(page_title="å»£å‘Šæˆæ•ˆå…¨èƒ½åˆ†æ v6.2 (Gemini 2.5 Pro)", layout="wide")

@st.cache_resource
def get_chinese_font():
    font_path = "NotoSansCJKtc-Regular.otf"
    url = "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/TraditionalChinese/NotoSansCJKtc-Regular.otf"
    if not os.path.exists(font_path):
        try:
            with st.spinner('æ­£åœ¨ä¸‹è¼‰ä¸­æ–‡å­—å‹ (é¦–æ¬¡åŸ·è¡Œéœ€æ™‚è¼ƒä¹…)...'):
                urllib.request.urlretrieve(url, font_path)
        except Exception as e:
            return None
    return fm.FontProperties(fname=font_path)

font_prop = get_chinese_font()

# ==========================================
# 2. æ ¸å¿ƒè¨ˆç®—é‚è¼¯
# ==========================================

def clean_ad_name(name):
    return re.sub(r' - è¤‡æœ¬.*$', '', str(name)).strip()

def create_summary_row(df, metric_cols):
    summary_dict = {}
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        summary_dict[col] = df[col].sum()
        
    for metric, (num, denom, is_pct) in metric_cols.items():
        total_num = summary_dict.get(num, 0)
        total_denom = summary_dict.get(denom, 0)
        if total_denom > 0:
            val = (total_num / total_denom)
            if is_pct: val *= 100
            summary_dict[metric] = round(val, 2)
        else:
            summary_dict[metric] = 0

    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns
    if len(non_numeric_cols) > 0:
        summary_dict[non_numeric_cols[0]] = 'å…¨å¸³æˆ¶å¹³å‡'
        for col in non_numeric_cols[1:]:
            summary_dict[col] = '-'
    return pd.DataFrame([summary_dict])

def calculate_consolidated_metrics(df_group, conv_col):
    df_metrics = df_group.agg({
        'èŠ±è²»é‡‘é¡ (TWD)': 'sum',
        conv_col: 'sum',
        'é€£çµé»æ“Šæ¬¡æ•¸': 'sum',
        'æ›å…‰æ¬¡æ•¸': 'sum'
    }).reset_index()

    df_metrics = df_metrics[df_metrics['èŠ±è²»é‡‘é¡ (TWD)'] > 0]

    df_metrics['CPA (TWD)'] = df_metrics.apply(lambda x: x['èŠ±è²»é‡‘é¡ (TWD)'] / x[conv_col] if x[conv_col] > 0 else 0, axis=1)
    df_metrics['CTR (%)'] = df_metrics.apply(lambda x: (x['é€£çµé»æ“Šæ¬¡æ•¸'] / x['æ›å…‰æ¬¡æ•¸']) * 100 if x['æ›å…‰æ¬¡æ•¸'] > 0 else 0, axis=1)
    df_metrics['CVR (%)'] = df_metrics.apply(lambda x: (x[conv_col] / x['é€£çµé»æ“Šæ¬¡æ•¸']) * 100 if x['é€£çµé»æ“Šæ¬¡æ•¸'] > 0 else 0, axis=1)
    
    df_metrics = df_metrics.round(2).sort_values(by='èŠ±è²»é‡‘é¡ (TWD)', ascending=False)

    metric_config = {
        'CPA (TWD)': ('èŠ±è²»é‡‘é¡ (TWD)', conv_col, False),
        'CTR (%)': ('é€£çµé»æ“Šæ¬¡æ•¸', 'æ›å…‰æ¬¡æ•¸', True),
        'CVR (%)': (conv_col, 'é€£çµé»æ“Šæ¬¡æ•¸', True)
    }
    summary_row = create_summary_row(df_metrics, metric_config)
    
    if not df_metrics.empty:
        return pd.concat([df_metrics, summary_row], ignore_index=True)
    else:
        return df_metrics

def collect_period_results(df, period_name_short, conv_col):
    df['å»£å‘Šåç¨±_clean'] = df['å»£å‘Šåç¨±'].apply(clean_ad_name)
    results = []
    
    # 0. è©³ç´°å±¤ç´š
    results.append((
        f'{period_name_short}_Detail_è©³ç´°(çµ„åˆ+å»£å‘Š)', 
        calculate_consolidated_metrics(df.groupby(['è¡ŒéŠ·æ´»å‹•åç¨±', 'å»£å‘Šçµ„åˆåç¨±', 'å»£å‘Šåç¨±']), conv_col)
    ))
    # 1. å»£å‘Šå±¤ç´š
    results.append((f'{period_name_short}_Ad_å»£å‘Š', calculate_consolidated_metrics(df.groupby('å»£å‘Šåç¨±_clean'), conv_col)))
    # 2. å»£å‘Šçµ„åˆå±¤ç´š
    results.append((f'{period_name_short}_AdSet_å»£å‘Šçµ„åˆ', calculate_consolidated_metrics(df.groupby(['è¡ŒéŠ·æ´»å‹•åç¨±', 'å»£å‘Šçµ„åˆåç¨±']), conv_col)))
    # 3. è¡ŒéŠ·æ´»å‹•å±¤ç´š
    results.append((f'{period_name_short}_Campaign_è¡ŒéŠ·æ´»å‹•', calculate_consolidated_metrics(df.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conv_col)))
    
    return results

# ==========================================
# 3. ç•°å¸¸åµæ¸¬èˆ‡è¶¨å‹¢åˆ†æé‚è¼¯
# ==========================================
def check_daily_anomalies(df_p1, df_p7, level_name='è¡ŒéŠ·æ´»å‹•åç¨±'):
    p1 = df_p1[df_p1[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    p7 = df_p7[df_p7[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    
    if p1.empty or p7.empty: return pd.DataFrame()

    merged = pd.merge(p1, p7, on=level_name, suffixes=('_P1', '_P7'), how='inner')
    alerts = []
    
    for _, row in merged.iterrows():
        if row['èŠ±è²»é‡‘é¡ (TWD)_P1'] < 200: continue 

        name = row[level_name]
        cpa_p1, cpa_p7 = row['CPA (TWD)_P1'], row['CPA (TWD)_P7']
        ctr_p1, ctr_p7 = row['CTR (%)_P1'], row['CTR (%)_P7']
        spend_p1 = row['èŠ±è²»é‡‘é¡ (TWD)_P1']

        if cpa_p7 > 0 and cpa_p1 > cpa_p7 * 1.3:
            diff = int(((cpa_p1 - cpa_p7) / cpa_p7) * 100)
            alerts.append({'å±¤ç´š': level_name, 'åç¨±': name, 'é¡å‹': 'ğŸ”´ CPA æš´æ¼²', 
                           'æ•¸æ“šå°æ¯”': f"æ˜¨${cpa_p1:.0f} vs å‡${cpa_p7:.0f} (ğŸ”º{diff}%)", 'å»ºè­°': 'æª¢æŸ¥ç«¶åƒ¹æˆ–å—çœ¾'})
            
        if ctr_p7 > 0 and ctr_p1 < ctr_p7 * 0.8:
            diff = int(((ctr_p7 - ctr_p1) / ctr_p7) * 100)
            alerts.append({'å±¤ç´š': level_name, 'åç¨±': name, 'é¡å‹': 'ğŸ“‰ CTR é©Ÿé™', 
                           'æ•¸æ“šå°æ¯”': f"æ˜¨{ctr_p1}% vs å‡{ctr_p7}% (ğŸ”»{diff}%)", 'å»ºè­°': 'ç´ æç–²ä¹/æ›´æ›ç´ æ'})
            
        if cpa_p1 == 0 and spend_p1 > 500:
             alerts.append({'å±¤ç´š': level_name, 'åç¨±': name, 'é¡å‹': 'ğŸ›‘ é«˜èŠ±è²»0è½‰æ›', 
                            'æ•¸æ“šå°æ¯”': f"æ˜¨èŠ±è²» ${spend_p1:.0f}", 'å»ºè­°': 'æª¢æŸ¥è½åœ°é /è¨­å®š'})

    return pd.DataFrame(alerts)

def check_weekly_trends(df_p7, df_pp7, level_name='è¡ŒéŠ·æ´»å‹•åç¨±'):
    curr = df_p7[df_p7[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    prev = df_pp7[df_pp7[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    
    if curr.empty or prev.empty: return pd.DataFrame()
    
    merged = pd.merge(curr, prev, on=level_name, suffixes=('_This', '_Last'), how='inner')
    trends = []
    
    for _, row in merged.iterrows():
        if row['èŠ±è²»é‡‘é¡ (TWD)_This'] < 1000: continue
        
        name = row[level_name]
        cpa_this, cpa_last = row['CPA (TWD)_This'], row['CPA (TWD)_Last']
        ctr_this, ctr_last = row['CTR (%)_This'], row['CTR (%)_Last']
        spend_this, spend_last = row['èŠ±è²»é‡‘é¡ (TWD)_This'], row['èŠ±è²»é‡‘é¡ (TWD)_Last']
        
        if cpa_last > 0 and cpa_this > cpa_last * 1.2:
            diff = int(((cpa_this - cpa_last) / cpa_last) * 100)
            trends.append({
                'å±¤ç´š': level_name, 'åç¨±': name, 'ç‹€æ…‹': 'âš ï¸ æˆæœ¬æƒ¡åŒ–',
                'æ•¸æ“šè®ŠåŒ–': f"${cpa_this:.0f} (vs ${cpa_last:.0f})",
                'è®ŠåŒ–å¹…åº¦': f"ğŸ”º +{diff}%",
                'è¨ºæ–·': 'ç«¶çˆ­åŠ åŠ‡æˆ–è½‰æ›ç‡ä¸‹é™'
            })
            
        if ctr_last > 0 and ctr_this < ctr_last * 0.85:
            diff = int(((ctr_last - ctr_this) / ctr_last) * 100)
            trends.append({
                'å±¤ç´š': level_name, 'åç¨±': name, 'ç‹€æ…‹': 'ğŸ“‰ CTR è¡°é€€',
                'æ•¸æ“šè®ŠåŒ–': f"{ctr_this}% (vs {ctr_last}%)",
                'è®ŠåŒ–å¹…åº¦': f"ğŸ”» -{diff}%",
                'è¨ºæ–·': 'ç´ æé–‹å§‹è€åŒ–'
            })

        if spend_last > 0 and spend_this > spend_last * 1.2:
            if cpa_last > 0 and cpa_this > cpa_last * 1.1:
                trends.append({
                    'å±¤ç´š': level_name, 'åç¨±': name, 'ç‹€æ…‹': 'ğŸ’¸ æ“´é‡æ•ˆç‡å·®',
                    'æ•¸æ“šè®ŠåŒ–': f"èŠ±è²»å¢è‡³ ${spend_this:,.0f}",
                    'è®ŠåŒ–å¹…åº¦': f"CPA äº¦æ¼²",
                    'è¨ºæ–·': 'é‚Šéš›æ•ˆæ‡‰éæ¸›ï¼Œå»ºè­°æš«åœåŠ ç¢¼'
                })

    return pd.DataFrame(trends)

def get_trend_data_excel(df_p30d, conv_col):
    trend_df = df_p30d.copy()
    acc_daily = trend_df.groupby(['å¤©æ•¸']).agg({
        'èŠ±è²»é‡‘é¡ (TWD)': 'sum', conv_col: 'sum', 'é€£çµé»æ“Šæ¬¡æ•¸': 'sum', 'æ›å…‰æ¬¡æ•¸': 'sum'
    }).reset_index()
    acc_daily['è¡ŒéŠ·æ´»å‹•åç¨±'] = 'ğŸ† æ•´é«”å¸³æˆ¶ (Account Overall)'
    final_trend = acc_daily[acc_daily['èŠ±è²»é‡‘é¡ (TWD)'] > 0]
    final_trend['CPA (TWD)'] = final_trend.apply(lambda x: x['èŠ±è²»é‡‘é¡ (TWD)'] / x[conv_col] if x[conv_col] > 0 else 0, axis=1)
    final_trend['å¤©æ•¸'] = final_trend['å¤©æ•¸'].dt.strftime('%Y-%m-%d')
    return final_trend.round(2)

# ä¿®æ”¹ï¼šExcel åŒ¯å‡ºå‡½æ•¸å¢åŠ  ai_response åƒæ•¸
def to_excel_single_sheet_stacked(dfs_list, prompt_text, ai_response=None):
    # æª¢æŸ¥ xlsxwriter å¼•æ“æ˜¯å¦å¯ç”¨
    engine = 'xlsxwriter' if HAS_XLSXWRITER else None
    if not engine:
        # å¦‚æœæ²’æœ‰ xlsxwriterï¼Œå›é€€åˆ°é è¨­æˆ–æ‹‹å‡ºè­¦å‘Š
        # é€™è£¡ç‚ºäº†ç°¡å–®ï¼Œæˆ‘å€‘å‡è¨­ä½¿ç”¨è€…æœƒå®‰è£ã€‚å¦‚æœçœŸçš„æ²’æœ‰ï¼Œpandas å¯èƒ½æœƒå ±éŒ¯æˆ–ä½¿ç”¨ openpyxl
        pass

    output = io.BytesIO()
    # ä½¿ç”¨ engine åƒæ•¸
    try:
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            workbook = writer.book
            sheet_name = 'ğŸ“˜_å®Œæ•´åˆ†æå ±å‘Š'
            ws = workbook.add_worksheet(sheet_name)
            writer.sheets[sheet_name] = ws
            
            fmt_prompt = workbook.add_format({'text_wrap': True, 'valign': 'top', 'font_size': 10, 'bg_color': '#F0F2F6'})
            fmt_ai_response = workbook.add_format({'text_wrap': True, 'valign': 'top', 'font_size': 11, 'bg_color': '#FFF8DC', 'border': 1})
            fmt_header = workbook.add_format({'bold': True, 'font_size': 14, 'font_color': '#0068C9'})
            fmt_table_header = workbook.add_format({'bold': True, 'bg_color': '#E6E6E6', 'border': 1})
            
            current_row = 0
            
            # 1. å¯«å…¥ AI åˆ†æçµæœ (å¦‚æœæœ‰çš„è©±)
            if ai_response:
                ws.merge_range('A1:K1', "ğŸ¤– Gemini AI å»£å‘Šè¨ºæ–·å ±å‘Š (AI Analysis Report)", fmt_header)
                current_row += 1
                # ä¼°ç®—è¡Œæ•¸ (ç²—ç•¥ä¼°è¨ˆæ¯è¡Œ 50 å­—)
                ai_lines = ai_response.count('\n') + (len(ai_response) // 50) + 2
                ws.merge_range(current_row, 0, current_row + ai_lines, 10, ai_response, fmt_ai_response)
                current_row += ai_lines + 2
            
            # 2. å¯«å…¥ System Prompt (ç•™åº•ç”¨)
            ws.merge_range(current_row, 0, current_row, 8, "ğŸ› ï¸ ç³»çµ±åˆ†ææŒ‡ä»¤ (System Prompt Log)", fmt_header)
            current_row += 1
            prompt_lines = prompt_text.count('\n') + 3
            ws.merge_range(current_row, 0, current_row + prompt_lines, 10, prompt_text, fmt_prompt)
            current_row += prompt_lines + 2
            
            # 3. å¯«å…¥æ‰€æœ‰æ•¸æ“šè¡¨
            for title, df in dfs_list:
                ws.write(current_row, 0, f"ğŸ“Œ Table: {title}", fmt_header)
                current_row += 1
                df.to_excel(writer, sheet_name=sheet_name, startrow=current_row, index=False)
                for col_num, value in enumerate(df.columns.values):
                    ws.write(current_row, col_num, value, fmt_table_header)
                current_row += len(df) + 4
                
            ws.set_column('A:A', 40)
            ws.set_column('B:Z', 15)
    except Exception as e:
        # å¦‚æœ Excel å¯«å…¥å¤±æ•— (ä¾‹å¦‚ç¼ºå°‘ xlsxwriter)ï¼Œå›å‚³ç©º byte æˆ–éŒ¯èª¤æç¤º
        return None
            
    output.seek(0)
    return output.getvalue()

# ==========================================
# 4. æ–°å¢åŠŸèƒ½ï¼šGemini AI åˆ†æä¸²æ¥ (é›™æ¨¡å¼ï¼šSDK / REST API)
# ==========================================

# æ–°å¢è¼”åŠ©å‡½æ•¸ï¼šå®‰å…¨åœ°å°‡ DataFrame è½‰æ›ç‚ºæ–‡å­—æ ¼å¼ï¼Œé¿å…ç¼ºå°‘ tabulate å ±éŒ¯
def safe_to_markdown(df):
    """
    å˜—è©¦ä½¿ç”¨ markdown æ ¼å¼ï¼Œå¦‚æœç¼ºå°‘ tabulate å¥—ä»¶ï¼Œå‰‡å›é€€åˆ° Pipe åˆ†éš”çš„ CSV æ ¼å¼ã€‚
    LLM éƒ½èƒ½ç†è§£é€™å…©ç¨®æ ¼å¼ã€‚
    """
    try:
        return df.to_markdown(index=False)
    except ImportError:
        # å¦‚æœæ²’æœ‰ tabulateï¼Œæ‰‹å‹•è½‰ç‚ºé¡ä¼¼ Markdown çš„æ ¼å¼ (Pipe åˆ†éš”)
        # é€™è£¡ä½¿ç”¨ to_csv ä¸¦ç”¨ '|' åˆ†éš”ï¼Œæ•ˆæœè·Ÿ Markdown å¾ˆåƒ
        return df.to_csv(sep='|', index=False)
    except Exception:
        # æœ€å¾Œçš„é˜²ç·šï¼šç›´æ¥è½‰å­—ä¸²
        return df.to_string(index=False)

def call_gemini_analysis(api_key, alerts_daily, alerts_weekly, campaign_summary):
    # æº–å‚™ Prompt (å…©ç¨®æ¨¡å¼å…±ç”¨)
    data_context = "\n\n# ğŸ“Š Account Data Summary\n"
    data_context += "## 1. Daily Alerts (P1D vs P7D Anomalies)\n"
    if not alerts_daily.empty:
        # ä½¿ç”¨å®‰å…¨çš„è½‰æ›å‡½æ•¸
        data_context += safe_to_markdown(alerts_daily)
    else:
        data_context += "No critical daily anomalies detected."
        
    data_context += "\n\n## 2. Weekly Trends (P7D vs PP7D Decline)\n"
    if not alerts_weekly.empty:
        # ä½¿ç”¨å®‰å…¨çš„è½‰æ›å‡½æ•¸
        data_context += safe_to_markdown(alerts_weekly)
    else:
        data_context += "No significant weekly decline trends detected."
        
    data_context += "\n\n## 3. Current Week Campaign Performance (P7D)\n"
    # ä½¿ç”¨å®‰å…¨çš„è½‰æ›å‡½æ•¸
    data_context += safe_to_markdown(campaign_summary.head(10))
    
    full_prompt = AI_CONSULTANT_PROMPT + data_context + "\n\n# User Request: è«‹æ ¹æ“šä¸Šè¿°æ•¸æ“šï¼Œç”¢ç”Ÿä¸€ä»½å»£å‘Šå„ªåŒ–è¨ºæ–·å ±å‘Šã€‚"

    with st.spinner('ğŸ¤– AI æ­£åœ¨åˆ†ææ•¸æ“šä¸­... (é€™å¯èƒ½éœ€è¦ 10-20 ç§’)'):
        try:
            # æ¨¡å¼ A: ä½¿ç”¨å®˜æ–¹ SDK (å¦‚æœå·²å®‰è£)
            if HAS_GENAI:
                genai.configure(api_key=api_key)
                # ä¿®æ”¹é»ï¼šæ›´æ›æ¨¡å‹ç‚º gemini-2.5-pro
                model = genai.GenerativeModel('gemini-2.5-pro')
                response = model.generate_content(full_prompt)
                return response.text
            
            # æ¨¡å¼ B: ä½¿ç”¨ REST API (Fallback æ¨¡å¼)
            else:
                # ä¿®æ”¹é»ï¼šæ›´æ›æ¨¡å‹ç‚º gemini-2.5-pro
                url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-pro:generateContent?key={api_key}"
                headers = {'Content-Type': 'application/json'}
                data = {
                    "contents": [{
                        "parts": [{"text": full_prompt}]
                    }]
                }
                
                response = requests.post(url, headers=headers, json=data)
                
                if response.status_code == 200:
                    result_json = response.json()
                    # å®‰å…¨åœ°è§£æ JSON å›å‚³çµæ§‹
                    try:
                        return result_json['candidates'][0]['content']['parts'][0]['text']
                    except (KeyError, IndexError):
                        return f"âš ï¸ API å›å‚³æ ¼å¼ä¸å¦‚é æœŸ: {str(result_json)}"
                else:
                    return f"âš ï¸ API é€£ç·šéŒ¯èª¤ ({response.status_code}): {response.text}"
                
        except Exception as e:
            return f"âŒ ç³»çµ±ç™¼ç”ŸéŒ¯èª¤: {str(e)}\nè«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢ºï¼Œæˆ–è©² Key æ˜¯å¦æœ‰æ¬Šé™å­˜å– 2.5 Pro æ¨¡å‹ã€‚"

# ==========================================
# 5. ä¸»ç¨‹å¼ UI
# ==========================================
st.title("ğŸ“Š å»£å‘Šæˆæ•ˆå…¨èƒ½åˆ†æ v6.2 (Gemini 2.5 Pro)")

# é¡¯ç¤ºç’°å¢ƒè­¦å‘Š (å¦‚æœç¼ºå°‘é—œéµå¥—ä»¶)
if not HAS_GENAI:
    st.warning("â„¹ï¸ æç¤ºï¼šæœªåµæ¸¬åˆ° `google-generativeai` å¥—ä»¶ã€‚ç³»çµ±å°‡è‡ªå‹•åˆ‡æ›ç‚º **REST API å…¼å®¹æ¨¡å¼** (åªéœ€ API Key å³å¯é‹ä½œ)ã€‚")
if not HAS_XLSXWRITER:
    st.warning("âš ï¸ è­¦å‘Šï¼šæœªåµæ¸¬åˆ° `xlsxwriter` å¥—ä»¶ã€‚Excel åŒ¯å‡ºåŠŸèƒ½å¯èƒ½æœƒå¤±æ•ˆã€‚")

# åˆå§‹åŒ– Session State
if 'gemini_result' not in st.session_state:
    st.session_state['gemini_result'] = None

uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ CSV å ±è¡¨æª”æ¡ˆ", type=['csv'])

if uploaded_file is not None:
    try:
        # 1. è®€å–èˆ‡æ¬„ä½åµæ¸¬
        try:
            df = pd.read_csv(uploaded_file, encoding='utf-8')
        except UnicodeDecodeError:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding='cp950')
        except Exception as e:
            st.error(f"æª”æ¡ˆè®€å–æœªçŸ¥çš„éŒ¯èª¤: {e}")
            st.stop()

        df.columns = df.columns.str.strip()
        all_columns = df.columns.tolist()
        
        with st.sidebar:
            st.header("âš™ï¸ åˆ†æè¨­å®š")
            
            st.subheader("ğŸ¤– AI åˆ†æè¨­å®š")
            gemini_api_key = st.text_input("Gemini API Key", type="password", placeholder="è¼¸å…¥ Key ä»¥å•Ÿç”¨ AI åˆ†æ")
            st.caption("[å–å¾— Google AI Studio Key](https://aistudio.google.com/app/apikey)")
            st.divider()
            
            suggested_idx = 0
            for idx, col in enumerate(all_columns):
                c_low = col.lower()
                if 'æˆæœ¬' in col or 'cost' in c_low: continue
                if ('free' in c_low and 'course' in c_low): suggested_idx = idx; break
                if 'è³¼è²·' in col or 'purchase' in c_low: suggested_idx = idx; break
                if 'è½‰æ›' in col: suggested_idx = idx; break
                
            conversion_col = st.selectbox("ğŸ¯ ç›®æ¨™è½‰æ›æ¬„ä½:", options=all_columns, index=suggested_idx)
            
            def find_col(opts, default):
                for opt in opts:
                    for col in all_columns:
                        if opt in col: return col
                return default

            spend_col = find_col(['èŠ±è²»é‡‘é¡ (TWD)', 'èŠ±è²»', 'é‡‘é¡'], 'èŠ±è²»é‡‘é¡ (TWD)')
            clicks_col = find_col(['é€£çµé»æ“Šæ¬¡æ•¸', 'é€£çµé»æ“Š'], 'é€£çµé»æ“Šæ¬¡æ•¸')
            impressions_col = find_col(['æ›å…‰æ¬¡æ•¸', 'æ›å…‰'], 'æ›å…‰æ¬¡æ•¸')

        # 2. æ•¸æ“šæ¸…æ´—
        cols_to_numeric = [spend_col, clicks_col, impressions_col, conversion_col]
        for col in cols_to_numeric:
            if col in df.columns:
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

        if 'å¤©æ•¸' not in df.columns:
             st.error("éŒ¯èª¤ï¼šCSV æª”æ¡ˆä¸­æ‰¾ä¸åˆ°ã€Œå¤©æ•¸ã€æ¬„ä½ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼ã€‚")
             st.stop()

        df['å¤©æ•¸'] = pd.to_datetime(df['å¤©æ•¸'], errors='coerce')
        df = df.dropna(subset=['å¤©æ•¸']) 

        df_std = df.rename(columns={
            spend_col: 'èŠ±è²»é‡‘é¡ (TWD)',
            clicks_col: 'é€£çµé»æ“Šæ¬¡æ•¸',
            impressions_col: 'æ›å…‰æ¬¡æ•¸'
        })
        
        # 3. æ—¥æœŸå€é–“èˆ‡è³‡æ–™åˆ†çµ„
        if df_std.empty:
            st.error("éŒ¯èª¤ï¼šè³‡æ–™ç¶“éæ¸…æ´—å¾Œç‚ºç©ºï¼Œè«‹æª¢æŸ¥åŸå§‹æª”æ¡ˆæ˜¯å¦åŒ…å«æœ‰æ•ˆçš„æ—¥æœŸèˆ‡æ•¸æ“šã€‚")
            st.stop()

        max_date = df_std['å¤©æ•¸'].max().normalize()
        today = max_date + timedelta(days=1)
        
        # P1D / P7D / PP7D / P30D
        p1d_start = max_date
        df_p1d = df_std[df_std['å¤©æ•¸'] == p1d_start].copy()
        
        p7d_start = today - timedelta(days=7)
        p7d_end = today - timedelta(days=1)
        pp7d_start = p7d_start - timedelta(days=7)
        pp7d_end = p7d_start - timedelta(days=1)
        p30d_start = today - timedelta(days=30)
        p30d_end = today - timedelta(days=1)
        
        df_p7d = df_std[(df_std['å¤©æ•¸'] >= p7d_start) & (df_std['å¤©æ•¸'] <= p7d_end)].copy()
        df_pp7d = df_std[(df_std['å¤©æ•¸'] >= pp7d_start) & (df_std['å¤©æ•¸'] <= pp7d_end)].copy()
        df_p30d = df_std[(df_std['å¤©æ•¸'] >= p30d_start) & (df_std['å¤©æ•¸'] <= p30d_end)].copy()
        
        res_p1d_camp = calculate_consolidated_metrics(df_p1d.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conversion_col)
        res_p7d_camp = calculate_consolidated_metrics(df_p7d.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conversion_col)
        res_pp7d_camp = calculate_consolidated_metrics(df_pp7d.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conversion_col)
        
        alerts_daily = check_daily_anomalies(res_p1d_camp, res_p7d_camp, 'è¡ŒéŠ·æ´»å‹•åç¨±')
        alerts_weekly = check_weekly_trends(res_p7d_camp, res_pp7d_camp, 'è¡ŒéŠ·æ´»å‹•åç¨±')

        # --- UI å‘ˆç¾ ---
        tab1, tab2, tab3 = st.tabs(["ğŸ“ˆ æˆ°æƒ…å®¤ & é›™é‡ç›£æ§", "ğŸ“‘ è©³ç´°æ•¸æ“šè¡¨ (AdSet+Ad)", "ğŸ¤– AI æ·±åº¦è¨ºæ–· (Gemini)"])
        
        with tab1:
            col_a, col_b = st.columns(2)
            with col_a:
                st.subheader("ğŸš¨ P1D ç·Šæ€¥è­¦ç¤º (æ˜¨æ—¥ vs å‡å€¼)")
                if not alerts_daily.empty:
                    st.dataframe(alerts_daily, hide_index=True, use_container_width=True)
                else:
                    st.success("æ˜¨æ—¥è¡¨ç¾å¹³ç©© (ç„¡ CPAæš´æ¼² / CTRé©Ÿé™)")
            
            with col_b:
                st.subheader("ğŸ“‰ P7D é€±ç’°æ¯”è¡°é€€ (æœ¬é€± vs ä¸Šé€±)")
                if not alerts_weekly.empty:
                    st.dataframe(alerts_weekly, hide_index=True, use_container_width=True)
                else:
                    st.info("æœ¬é€±ç„¡é¡¯è‘—è¡°é€€é …ç›® (CPAèˆ‡CTRçš†ç©©å®š)")

            st.divider()
            # 30æ—¥æ¦‚æ³
            total_spend = df_p30d['èŠ±è²»é‡‘é¡ (TWD)'].sum()
            total_conv = df_p30d[conversion_col].sum()
            cpa_30d = total_spend / total_conv if total_conv > 0 else 0
            
            c1, c2, c3 = st.columns(3)
            c1.metric("è¿‘30æ—¥ç¸½èŠ±è²»", f"${total_spend:,.0f}")
            c2.metric(f"è¿‘30æ—¥ç¸½è½‰æ›", f"{total_conv:,.0f}")
            c3.metric("è¿‘30æ—¥å¹³å‡ CPA", f"${cpa_30d:,.0f}")
            
            # è¶¨å‹¢åœ–
            daily = df_p30d.groupby('å¤©æ•¸')[['èŠ±è²»é‡‘é¡ (TWD)', conversion_col, 'é€£çµé»æ“Šæ¬¡æ•¸', 'æ›å…‰æ¬¡æ•¸']].sum().reset_index()
            daily['æ—¥æœŸstr'] = daily['å¤©æ•¸'].dt.strftime('%m-%d')
            
            fig, ax1 = plt.subplots(figsize=(12, 5))
            ax2 = ax1.twinx()
            ax1.bar(daily['æ—¥æœŸstr'], daily['èŠ±è²»é‡‘é¡ (TWD)'], color='#ddd', label='èŠ±è²»', alpha=0.6)
            ax2.plot(daily['æ—¥æœŸstr'], daily[conversion_col], color='red', marker='o', label='è½‰æ›æ•¸', linewidth=2)
            ax1.set_xlabel('æ—¥æœŸ', fontproperties=font_prop)
            ax1.set_ylabel('èŠ±è²» (TWD)', fontproperties=font_prop)
            ax2.set_ylabel('è½‰æ›æ•¸', fontproperties=font_prop)
            if font_prop:
                for label in ax1.get_xticklabels(): label.set_fontproperties(font_prop)
            st.pyplot(fig)

        with tab2:
            st.markdown("### ğŸ” å„å€é–“è©³ç´°æ•¸æ“š (è¡ŒéŠ·æ´»å‹• > å»£å‘Šçµ„åˆ > å»£å‘Š)")
            t_p1, t_p7, t_pp7, t_p30 = st.tabs(["P1D (æ˜¨æ—¥)", "P7D (æœ¬é€±)", "PP7D (ä¸Šé€±)", "P30D (æœˆå ±)"])
            
            res_p1 = collect_period_results(df_p1d, 'P1D', conversion_col)
            res_p7 = collect_period_results(df_p7d, 'P7D', conversion_col)
            res_pp7 = collect_period_results(df_pp7d, 'PP7D', conversion_col)
            res_p30 = collect_period_results(df_p30d, 'P30D', conversion_col)
            
            def render_data_tab(results_list, unique_key):
                st.info("ğŸ’¡ ä¸‹è¡¨å·²å±•é–‹ç‚ºã€Œè©³ç´°å±¤ç´šã€ï¼Œæ‚¨å¯çœ‹åˆ°æ¯å€‹è¡ŒéŠ·æ´»å‹• > å»£å‘Šçµ„åˆ ä¸‹çš„å„åˆ¥å»£å‘Šè¡¨ç¾ã€‚")
                st.dataframe(results_list[0][1], use_container_width=True)
                
                with st.expander("æŸ¥çœ‹å…¶ä»–åŒ¯ç¸½å±¤ç´š (è¡ŒéŠ·æ´»å‹• / å»£å‘Šçµ„åˆ / å»£å‘Šæ•´é«”)"):
                    view_mode = st.radio(
                        "é¸æ“‡å…¶ä»–æª¢è¦–å±¤ç´š:", 
                        ["è¡ŒéŠ·æ´»å‹• (Campaign)", "å»£å‘Šçµ„åˆ (AdSet)", "å»£å‘Š (Ad)"],
                        horizontal=True,
                        key=unique_key
                    )
                    if view_mode == "è¡ŒéŠ·æ´»å‹• (Campaign)":
                        st.dataframe(results_list[3][1], use_container_width=True)
                    elif view_mode == "å»£å‘Šçµ„åˆ (AdSet)":
                        st.dataframe(results_list[2][1], use_container_width=True)
                    elif view_mode == "å»£å‘Š (Ad)":
                        st.dataframe(results_list[1][1], use_container_width=True)

            with t_p1: render_data_tab(res_p1, "radio_p1")
            with t_p7: render_data_tab(res_p7, "radio_p7")
            with t_pp7: render_data_tab(res_pp7, "radio_pp7")
            with t_p30: render_data_tab(res_p30, "radio_p30")

        # === Tab 3: AI åˆ†æå€å¡Š ===
        with tab3:
            st.header("ğŸ¤– Gemini AI å»£å‘Šæˆæ•ˆè¨ºæ–·")
            st.markdown("""
            AI å°‡æ ¹æ“š **æ¯æ—¥è­¦ç¤º (Daily Alerts)**ã€**é€±è¶¨å‹¢ (Weekly Trends)** èˆ‡ **æœ¬é€±è¡ŒéŠ·æ´»å‹• (P7D Campaign)** æ•¸æ“šï¼Œ
            è‡ªå‹•ä¾ç…§å·¦å´è¨­å®šçš„ã€ŒAI é¡§å•æŒ‡ä»¤ã€é€²è¡Œè¨ºæ–·ä¸¦æä¾›å„ªåŒ–å»ºè­°ã€‚
            """)
            
            col_ai_btn, col_ai_warn = st.columns([1, 2])
            with col_ai_btn:
                # å³ä½¿æ²’å®‰è£å¥—ä»¶ï¼Œç¾åœ¨ä¹Ÿå…è¨±æŒ‰ä¸‹æŒ‰éˆ•ï¼ˆæœƒä½¿ç”¨ REST API Fallbackï¼‰
                run_ai = st.button("ğŸš€ é–‹å§‹ AI æ™ºèƒ½åˆ†æ", type="primary")
            
            if run_ai:
                if not gemini_api_key:
                    st.warning("âš ï¸ è«‹å…ˆæ–¼å·¦å´å´é‚Šæ¬„è¼¸å…¥ Gemini API Key")
                else:
                    analysis_result = call_gemini_analysis(
                        gemini_api_key, 
                        alerts_daily, 
                        alerts_weekly, 
                        res_p7d_camp
                    )
                    # é—œéµï¼šå°‡çµæœå­˜å…¥ Session Stateï¼Œç¢ºä¿åˆ‡æ› Tab æˆ–é»æ“Šä¸‹è¼‰æ™‚å…§å®¹ä¸æ¶ˆå¤±
                    st.session_state['gemini_result'] = analysis_result
            
            # é¡¯ç¤ºåˆ†æçµæœ (å¦‚æœå­˜åœ¨)
            if st.session_state['gemini_result']:
                 st.markdown("### ğŸ“ AI è¨ºæ–·å ±å‘Š")
                 st.markdown("---")
                 st.markdown(st.session_state['gemini_result'])

        # ä¸‹è¼‰å€ (ç¶­æŒä¸¦å¢å¼·åŠŸèƒ½)
        with st.sidebar:
            st.divider()
            excel_stack = []
            excel_stack.append(('Trend_Daily', get_trend_data_excel(df_p30d, conversion_col)))
            excel_stack.extend(res_p1)
            excel_stack.extend(res_p7)
            excel_stack.extend(res_pp7)
            excel_stack.extend(res_p30)
            
            # å¾ Session State ç²å–æœ€æ–°çš„ AI åˆ†æçµæœ (å¦‚æœæœ‰çš„è©±)
            current_ai_result = st.session_state.get('gemini_result', None)
            
            # å‚³å…¥ AI çµæœåˆ° Excel ç”Ÿæˆå‡½æ•¸
            excel_bytes = to_excel_single_sheet_stacked(excel_stack, AI_CONSULTANT_PROMPT, current_ai_result)
            
            if excel_bytes:
                button_label = "ğŸ“¥ ä¸‹è¼‰å®Œæ•´åˆ†æå ±è¡¨"
                if current_ai_result:
                    button_label += " (å·²åŒ…å« AI è¨ºæ–·)"
                
                st.download_button(
                    label=button_label,
                    data=excel_bytes,
                    file_name=f"Full_Report_{max_date.strftime('%Y%m%d')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                )
            else:
                st.error("Excel ç”¢ç”Ÿå¤±æ•—ï¼Œè«‹æª¢æŸ¥ xlsxwriter å¥—ä»¶æ˜¯å¦å®‰è£ã€‚")

    except Exception as e:
        st.error(f"ç³»çµ±ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
        st.write("å»ºè­°æª¢æŸ¥ï¼š1. CSVæ ¼å¼æ˜¯å¦æ­£ç¢º 2. æ˜¯å¦åŒ…å«è½‰æ›/èŠ±è²»æ¬„ä½")
