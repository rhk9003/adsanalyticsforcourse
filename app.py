import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import os
import urllib.request
import re
from datetime import datetime, timedelta
import io

# ==========================================
# 0. å…¨åŸŸè¨­å®šï¼šAI é¡§å•æŒ‡ä»¤
# ==========================================
AI_CONSULTANT_PROMPT = """
# âš ï¸ SYSTEM OVERRIDE: DATA LOADING INSTRUCTION
**To the AI Analysis Model:**
The user has uploaded a **Single-Sheet Excel File**.
1. **ALL DATA** is contained in the **First Sheet**.
2. **Key Comparisons**: 
   - **P1D vs P7D**: Immediate daily alerts.
   - **P7D vs PP7D**: Week-over-Week trend analysis.

# Role
ä½ æ˜¯ä¸€ä½è³‡æ·±æˆæ•ˆå»£å‘Šåˆ†æå¸«ã€‚

# Analysis Requirements
## 1. ğŸš¨ P1D ç·Šæ€¥ç•°å¸¸ (Daily Alert)
- æª¢æŸ¥ **P1D (æ˜¨æ—¥)** ç›¸è¼ƒæ–¼ **P7D (å‡å€¼)** æ˜¯å¦æœ‰ CPA æš´æ¼² (>30%) æˆ– CTR é©Ÿé™ (>20%)ã€‚
- é€™æ˜¯ã€Œæ•‘ç«ã€å±¤ç´šï¼Œè«‹å„ªå…ˆæŒ‡å‡ºéœ€è¦ç«‹å³é—œé–‰æˆ–æª¢æŸ¥çš„å»£å‘Šã€‚

## 2. ğŸ“‰ P7D vs PP7D é€±ç’°æ¯”åˆ†æ (WoW Trend)
- å°æ¯” **P7D (æœ¬é€±)** èˆ‡ **PP7D (ä¸Šé€±)**ã€‚
- æ‰¾å‡º CPA è®Šé«˜ã€CVR è®Šä½çš„ã€Œè¡°é€€è¡ŒéŠ·æ´»å‹•ã€ã€‚
- è‹¥æœ¬é€±èŠ±è²»å¢åŠ ä½† ROAS/CPA è®Šå·®ï¼Œè«‹æ¨™è¨˜ç‚ºã€Œæ“´é‡å¤±æ•— (Inefficient Scaling)ã€ã€‚
- è‹¥æœ¬é€± CTR æå‡ä½† CVR ä¸‹é™ï¼Œè«‹æ¨™è¨˜ç‚ºã€Œæµé‡å“è³ªè®Šå·® (Traffic Quality Drop)ã€ã€‚

## 3. ç¶œåˆå„ªåŒ–å»ºè­°
- é‡å°è¡°é€€é …ç›®æå‡ºå…·é«”å‡è¨­ï¼ˆç´ æç–²ä¹ï¼Ÿç«¶åƒ¹æ¿€çƒˆï¼Ÿå—çœ¾é£½å’Œï¼Ÿï¼‰ã€‚
"""

# ==========================================
# 1. åŸºç¤è¨­å®šèˆ‡å­—å‹è™•ç†
# ==========================================
st.set_page_config(page_title="å»£å‘Šæˆæ•ˆå…¨èƒ½åˆ†æ v5.5 (è©³ç´°å±¤ç´šç‰ˆ)", layout="wide")

@st.cache_resource
def get_chinese_font():
    font_path = "NotoSansCJKtc-Regular.otf"
    url = "https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTF/TraditionalChinese/NotoSansCJKtc-Regular.otf"
    if not os.path.exists(font_path):
        try:
            with st.spinner('æ­£åœ¨ä¸‹è¼‰ä¸­æ–‡å­—å‹ (é¦–æ¬¡åŸ·è¡Œéœ€æ™‚è¼ƒä¹…)...'):
                urllib.request.urlretrieve(url, font_path)
        except Exception as e:
            return None
    return fm.FontProperties(fname=font_path)

font_prop = get_chinese_font()

# ==========================================
# 2. æ ¸å¿ƒè¨ˆç®—é‚è¼¯
# ==========================================

def clean_ad_name(name):
    return re.sub(r' - è¤‡æœ¬.*$', '', str(name)).strip()

def create_summary_row(df, metric_cols):
    summary_dict = {}
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        summary_dict[col] = df[col].sum()
        
    for metric, (num, denom, is_pct) in metric_cols.items():
        total_num = summary_dict.get(num, 0)
        total_denom = summary_dict.get(denom, 0)
        if total_denom > 0:
            val = (total_num / total_denom)
            if is_pct: val *= 100
            summary_dict[metric] = round(val, 2)
        else:
            summary_dict[metric] = 0

    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns
    if len(non_numeric_cols) > 0:
        summary_dict[non_numeric_cols[0]] = 'å…¨å¸³æˆ¶å¹³å‡'
        for col in non_numeric_cols[1:]:
            summary_dict[col] = '-'
    return pd.DataFrame([summary_dict])

def calculate_consolidated_metrics(df_group, conv_col):
    df_metrics = df_group.agg({
        'èŠ±è²»é‡‘é¡ (TWD)': 'sum',
        conv_col: 'sum',
        'é€£çµé»æ“Šæ¬¡æ•¸': 'sum',
        'æ›å…‰æ¬¡æ•¸': 'sum'
    }).reset_index()

    df_metrics = df_metrics[df_metrics['èŠ±è²»é‡‘é¡ (TWD)'] > 0]

    df_metrics['CPA (TWD)'] = df_metrics.apply(lambda x: x['èŠ±è²»é‡‘é¡ (TWD)'] / x[conv_col] if x[conv_col] > 0 else 0, axis=1)
    df_metrics['CTR (%)'] = df_metrics.apply(lambda x: (x['é€£çµé»æ“Šæ¬¡æ•¸'] / x['æ›å…‰æ¬¡æ•¸']) * 100 if x['æ›å…‰æ¬¡æ•¸'] > 0 else 0, axis=1)
    df_metrics['CVR (%)'] = df_metrics.apply(lambda x: (x[conv_col] / x['é€£çµé»æ“Šæ¬¡æ•¸']) * 100 if x['é€£çµé»æ“Šæ¬¡æ•¸'] > 0 else 0, axis=1)
    
    df_metrics = df_metrics.round(2).sort_values(by='èŠ±è²»é‡‘é¡ (TWD)', ascending=False)

    metric_config = {
        'CPA (TWD)': ('èŠ±è²»é‡‘é¡ (TWD)', conv_col, False),
        'CTR (%)': ('é€£çµé»æ“Šæ¬¡æ•¸', 'æ›å…‰æ¬¡æ•¸', True),
        'CVR (%)': (conv_col, 'é€£çµé»æ“Šæ¬¡æ•¸', True)
    }
    summary_row = create_summary_row(df_metrics, metric_config)
    
    if not df_metrics.empty:
        return pd.concat([df_metrics, summary_row], ignore_index=True)
    else:
        return df_metrics

def collect_period_results(df, period_name_short, conv_col):
    df['å»£å‘Šåç¨±_clean'] = df['å»£å‘Šåç¨±'].apply(clean_ad_name)
    results = []
    # 0. å»£å‘Šå±¤ç´š (èšåˆç›¸åŒåç¨±çš„å»£å‘Š)
    results.append((f'{period_name_short}_Ad_å»£å‘Š', calculate_consolidated_metrics(df.groupby('å»£å‘Šåç¨±_clean'), conv_col)))
    # 1. å»£å‘Šçµ„åˆå±¤ç´š
    results.append((f'{period_name_short}_AdSet_å»£å‘Šçµ„åˆ', calculate_consolidated_metrics(df.groupby(['è¡ŒéŠ·æ´»å‹•åç¨±', 'å»£å‘Šçµ„åˆåç¨±']), conv_col)))
    # 2. è¡ŒéŠ·æ´»å‹•å±¤ç´š
    results.append((f'{period_name_short}_Campaign_è¡ŒéŠ·æ´»å‹•', calculate_consolidated_metrics(df.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conv_col)))
    # 3. [NEW] è©³ç´°å±¤ç´š (è¡ŒéŠ·æ´»å‹• > å»£å‘Šçµ„åˆ > å»£å‘Š)
    results.append((f'{period_name_short}_Detail_è©³ç´°(çµ„åˆ+å»£å‘Š)', calculate_consolidated_metrics(df.groupby(['è¡ŒéŠ·æ´»å‹•åç¨±', 'å»£å‘Šçµ„åˆåç¨±', 'å»£å‘Šåç¨±']), conv_col)))
    return results

# ==========================================
# 3. ç•°å¸¸åµæ¸¬èˆ‡è¶¨å‹¢åˆ†æé‚è¼¯
# ==========================================

def check_daily_anomalies(df_p1, df_p7, level_name='è¡ŒéŠ·æ´»å‹•åç¨±'):
    p1 = df_p1[df_p1[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    p7 = df_p7[df_p7[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    
    if p1.empty or p7.empty: return pd.DataFrame()

    merged = pd.merge(p1, p7, on=level_name, suffixes=('_P1', '_P7'), how='inner')
    alerts = []
    
    for _, row in merged.iterrows():
        if row['èŠ±è²»é‡‘é¡ (TWD)_P1'] < 200: continue 

        name = row[level_name]
        cpa_p1, cpa_p7 = row['CPA (TWD)_P1'], row['CPA (TWD)_P7']
        ctr_p1, ctr_p7 = row['CTR (%)_P1'], row['CTR (%)_P7']
        spend_p1 = row['èŠ±è²»é‡‘é¡ (TWD)_P1']

        if cpa_p7 > 0 and cpa_p1 > cpa_p7 * 1.3:
            diff = int(((cpa_p1 - cpa_p7) / cpa_p7) * 100)
            alerts.append({'å±¤ç´š': level_name, 'åç¨±': name, 'é¡å‹': 'ğŸ”´ CPA æš´æ¼²', 
                           'æ•¸æ“šå°æ¯”': f"æ˜¨${cpa_p1:.0f} vs å‡${cpa_p7:.0f} (ğŸ”º{diff}%)", 'å»ºè­°': 'æª¢æŸ¥ç«¶åƒ¹æˆ–å—çœ¾'})
            
        if ctr_p7 > 0 and ctr_p1 < ctr_p7 * 0.8:
            diff = int(((ctr_p7 - ctr_p1) / ctr_p7) * 100)
            alerts.append({'å±¤ç´š': level_name, 'åç¨±': name, 'é¡å‹': 'ğŸ“‰ CTR é©Ÿé™', 
                           'æ•¸æ“šå°æ¯”': f"æ˜¨{ctr_p1}% vs å‡{ctr_p7}% (ğŸ”»{diff}%)", 'å»ºè­°': 'ç´ æç–²ä¹/æ›´æ›ç´ æ'})
            
        if cpa_p1 == 0 and spend_p1 > 500:
             alerts.append({'å±¤ç´š': level_name, 'åç¨±': name, 'é¡å‹': 'ğŸ›‘ é«˜èŠ±è²»0è½‰æ›', 
                            'æ•¸æ“šå°æ¯”': f"æ˜¨èŠ±è²» ${spend_p1:.0f}", 'å»ºè­°': 'æª¢æŸ¥è½åœ°é /è¨­å®š'})

    return pd.DataFrame(alerts)

def check_weekly_trends(df_p7, df_pp7, level_name='è¡ŒéŠ·æ´»å‹•åç¨±'):
    curr = df_p7[df_p7[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    prev = df_pp7[df_pp7[level_name] != 'å…¨å¸³æˆ¶å¹³å‡'].copy()
    
    if curr.empty or prev.empty: return pd.DataFrame()
    
    merged = pd.merge(curr, prev, on=level_name, suffixes=('_This', '_Last'), how='inner')
    trends = []
    
    for _, row in merged.iterrows():
        if row['èŠ±è²»é‡‘é¡ (TWD)_This'] < 1000: continue
        
        name = row[level_name]
        cpa_this, cpa_last = row['CPA (TWD)_This'], row['CPA (TWD)_Last']
        ctr_this, ctr_last = row['CTR (%)_This'], row['CTR (%)_Last']
        spend_this, spend_last = row['èŠ±è²»é‡‘é¡ (TWD)_This'], row['èŠ±è²»é‡‘é¡ (TWD)_Last']
        
        if cpa_last > 0 and cpa_this > cpa_last * 1.2:
            diff = int(((cpa_this - cpa_last) / cpa_last) * 100)
            trends.append({
                'å±¤ç´š': level_name, 'åç¨±': name, 'ç‹€æ…‹': 'âš ï¸ æˆæœ¬æƒ¡åŒ–',
                'æ•¸æ“šè®ŠåŒ–': f"${cpa_this:.0f} (vs ${cpa_last:.0f})",
                'è®ŠåŒ–å¹…åº¦': f"ğŸ”º +{diff}%",
                'è¨ºæ–·': 'ç«¶çˆ­åŠ åŠ‡æˆ–è½‰æ›ç‡ä¸‹é™'
            })
            
        if ctr_last > 0 and ctr_this < ctr_last * 0.85:
            diff = int(((ctr_last - ctr_this) / ctr_last) * 100)
            trends.append({
                'å±¤ç´š': level_name, 'åç¨±': name, 'ç‹€æ…‹': 'ğŸ“‰ CTR è¡°é€€',
                'æ•¸æ“šè®ŠåŒ–': f"{ctr_this}% (vs {ctr_last}%)",
                'è®ŠåŒ–å¹…åº¦': f"ğŸ”» -{diff}%",
                'è¨ºæ–·': 'ç´ æé–‹å§‹è€åŒ–'
            })

        if spend_last > 0 and spend_this > spend_last * 1.2:
            if cpa_last > 0 and cpa_this > cpa_last * 1.1:
                trends.append({
                    'å±¤ç´š': level_name, 'åç¨±': name, 'ç‹€æ…‹': 'ğŸ’¸ æ“´é‡æ•ˆç‡å·®',
                    'æ•¸æ“šè®ŠåŒ–': f"èŠ±è²»å¢è‡³ ${spend_this:,.0f}",
                    'è®ŠåŒ–å¹…åº¦': f"CPA äº¦æ¼²",
                    'è¨ºæ–·': 'é‚Šéš›æ•ˆæ‡‰éæ¸›ï¼Œå»ºè­°æš«åœåŠ ç¢¼'
                })

    return pd.DataFrame(trends)

def get_trend_data_excel(df_p30d, conv_col):
    trend_df = df_p30d.copy()
    acc_daily = trend_df.groupby(['å¤©æ•¸']).agg({
        'èŠ±è²»é‡‘é¡ (TWD)': 'sum', conv_col: 'sum', 'é€£çµé»æ“Šæ¬¡æ•¸': 'sum', 'æ›å…‰æ¬¡æ•¸': 'sum'
    }).reset_index()
    acc_daily['è¡ŒéŠ·æ´»å‹•åç¨±'] = 'ğŸ† æ•´é«”å¸³æˆ¶ (Account Overall)'
    final_trend = acc_daily[acc_daily['èŠ±è²»é‡‘é¡ (TWD)'] > 0]
    final_trend['CPA (TWD)'] = final_trend.apply(lambda x: x['èŠ±è²»é‡‘é¡ (TWD)'] / x[conv_col] if x[conv_col] > 0 else 0, axis=1)
    final_trend['å¤©æ•¸'] = final_trend['å¤©æ•¸'].dt.strftime('%Y-%m-%d')
    return final_trend.round(2)

def to_excel_single_sheet_stacked(dfs_list, prompt_text):
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        workbook = writer.book
        sheet_name = 'ğŸ“˜_å®Œæ•´åˆ†æå ±å‘Š'
        ws = workbook.add_worksheet(sheet_name)
        writer.sheets[sheet_name] = ws
        
        fmt_prompt = workbook.add_format({'text_wrap': True, 'valign': 'top', 'font_size': 11, 'bg_color': '#F0F2F6'})
        fmt_header = workbook.add_format({'bold': True, 'font_size': 14, 'font_color': '#0068C9'})
        fmt_table_header = workbook.add_format({'bold': True, 'bg_color': '#E6E6E6', 'border': 1})
        
        current_row = 0
        ws.merge_range('A1:H1', "ğŸ¤– AI åˆ†æé¡§å•æŒ‡ä»¤ (SYSTEM PROMPT)", fmt_header)
        current_row += 1
        prompt_lines = prompt_text.count('\n') + 5
        ws.merge_range(current_row, 0, current_row + prompt_lines, 10, prompt_text, fmt_prompt)
        current_row += prompt_lines + 2
        
        for title, df in dfs_list:
            ws.write(current_row, 0, f"ğŸ“Œ Table: {title}", fmt_header)
            current_row += 1
            df.to_excel(writer, sheet_name=sheet_name, startrow=current_row, index=False)
            for col_num, value in enumerate(df.columns.values):
                ws.write(current_row, col_num, value, fmt_table_header)
            current_row += len(df) + 4
            
        ws.set_column('A:A', 40)
        ws.set_column('B:Z', 15)
            
    output.seek(0)
    return output.getvalue()

# ==========================================
# 4. ä¸»ç¨‹å¼ UI
# ==========================================
st.title("ğŸ“Š å»£å‘Šæˆæ•ˆå…¨èƒ½åˆ†æ v5.5 (è©³ç´°å±¤ç´šç‰ˆ)")

uploaded_file = st.file_uploader("è«‹ä¸Šå‚³ CSV å ±è¡¨æª”æ¡ˆ", type=['csv'])

if uploaded_file is not None:
    try:
        # 1. è®€å–èˆ‡æ¬„ä½åµæ¸¬
        try:
            df = pd.read_csv(uploaded_file, encoding='utf-8')
        except UnicodeDecodeError:
            uploaded_file.seek(0)
            df = pd.read_csv(uploaded_file, encoding='cp950')
        except Exception as e:
            st.error(f"æª”æ¡ˆè®€å–æœªçŸ¥çš„éŒ¯èª¤: {e}")
            st.stop()

        df.columns = df.columns.str.strip()
        all_columns = df.columns.tolist()
        
        with st.sidebar:
            st.header("âš™ï¸ åˆ†æè¨­å®š")
            suggested_idx = 0
            for idx, col in enumerate(all_columns):
                c_low = col.lower()
                if 'æˆæœ¬' in col or 'cost' in c_low: continue
                if ('free' in c_low and 'course' in c_low): suggested_idx = idx; break
                if 'è³¼è²·' in col or 'purchase' in c_low: suggested_idx = idx; break
                if 'è½‰æ›' in col: suggested_idx = idx; break
                
            conversion_col = st.selectbox("ğŸ¯ ç›®æ¨™è½‰æ›æ¬„ä½:", options=all_columns, index=suggested_idx)
            
            def find_col(opts, default):
                for opt in opts:
                    for col in all_columns:
                        if opt in col: return col
                return default

            spend_col = find_col(['èŠ±è²»é‡‘é¡ (TWD)', 'èŠ±è²»', 'é‡‘é¡'], 'èŠ±è²»é‡‘é¡ (TWD)')
            clicks_col = find_col(['é€£çµé»æ“Šæ¬¡æ•¸', 'é€£çµé»æ“Š'], 'é€£çµé»æ“Šæ¬¡æ•¸')
            impressions_col = find_col(['æ›å…‰æ¬¡æ•¸', 'æ›å…‰'], 'æ›å…‰æ¬¡æ•¸')

        # 2. æ•¸æ“šæ¸…æ´—
        cols_to_numeric = [spend_col, clicks_col, impressions_col, conversion_col]
        for col in cols_to_numeric:
            if col in df.columns:
                if df[col].dtype == 'object':
                    df[col] = df[col].astype(str).str.replace(',', '', regex=False)
                df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)

        if 'å¤©æ•¸' not in df.columns:
             st.error("éŒ¯èª¤ï¼šCSV æª”æ¡ˆä¸­æ‰¾ä¸åˆ°ã€Œå¤©æ•¸ã€æ¬„ä½ï¼Œè«‹æª¢æŸ¥æª”æ¡ˆæ ¼å¼ã€‚")
             st.stop()

        df['å¤©æ•¸'] = pd.to_datetime(df['å¤©æ•¸'], errors='coerce')
        df = df.dropna(subset=['å¤©æ•¸']) 

        df_std = df.rename(columns={
            spend_col: 'èŠ±è²»é‡‘é¡ (TWD)',
            clicks_col: 'é€£çµé»æ“Šæ¬¡æ•¸',
            impressions_col: 'æ›å…‰æ¬¡æ•¸'
        })
        
        # 3. æ—¥æœŸå€é–“èˆ‡è³‡æ–™åˆ†çµ„
        if df_std.empty:
            st.error("éŒ¯èª¤ï¼šè³‡æ–™ç¶“éæ¸…æ´—å¾Œç‚ºç©ºï¼Œè«‹æª¢æŸ¥åŸå§‹æª”æ¡ˆæ˜¯å¦åŒ…å«æœ‰æ•ˆçš„æ—¥æœŸèˆ‡æ•¸æ“šã€‚")
            st.stop()

        max_date = df_std['å¤©æ•¸'].max().normalize()
        today = max_date + timedelta(days=1)
        
        # P1D
        p1d_start = max_date
        df_p1d = df_std[df_std['å¤©æ•¸'] == p1d_start].copy()
        
        # P7D & PP7D
        p7d_start = today - timedelta(days=7)
        p7d_end = today - timedelta(days=1)
        pp7d_start = p7d_start - timedelta(days=7)
        pp7d_end = p7d_start - timedelta(days=1)
        p30d_start = today - timedelta(days=30)
        p30d_end = today - timedelta(days=1)
        
        df_p7d = df_std[(df_std['å¤©æ•¸'] >= p7d_start) & (df_std['å¤©æ•¸'] <= p7d_end)].copy()
        df_pp7d = df_std[(df_std['å¤©æ•¸'] >= pp7d_start) & (df_std['å¤©æ•¸'] <= pp7d_end)].copy()
        df_p30d = df_std[(df_std['å¤©æ•¸'] >= p30d_start) & (df_std['å¤©æ•¸'] <= p30d_end)].copy()
        
        # è¨ˆç®—å„å€é–“æ•¸æ“š
        res_p1d_camp = calculate_consolidated_metrics(df_p1d.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conversion_col)
        res_p7d_camp = calculate_consolidated_metrics(df_p7d.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conversion_col)
        res_pp7d_camp = calculate_consolidated_metrics(df_pp7d.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'), conversion_col)
        
        # === æ ¸å¿ƒï¼šç”¢ç”Ÿè­¦ç¤ºè¡¨ ===
        alerts_daily = check_daily_anomalies(res_p1d_camp, res_p7d_camp, 'è¡ŒéŠ·æ´»å‹•åç¨±')
        alerts_weekly = check_weekly_trends(res_p7d_camp, res_pp7d_camp, 'è¡ŒéŠ·æ´»å‹•åç¨±')

        # --- UI å‘ˆç¾ ---
        tab1, tab2 = st.tabs(["ğŸ“ˆ æˆ°æƒ…å®¤ & é›™é‡ç›£æ§", "ğŸ“‘ è©³ç´°æ•¸æ“šè¡¨ (å¯åˆ‡æ›å±¤ç´š)"])
        
        with tab1:
            col_a, col_b = st.columns(2)
            
            with col_a:
                st.subheader("ğŸš¨ P1D ç·Šæ€¥è­¦ç¤º (æ˜¨æ—¥ vs å‡å€¼)")
                if not alerts_daily.empty:
                    st.dataframe(alerts_daily, hide_index=True, use_container_width=True)
                else:
                    st.success("æ˜¨æ—¥è¡¨ç¾å¹³ç©© (ç„¡ CPAæš´æ¼² / CTRé©Ÿé™)")
            
            with col_b:
                st.subheader("ğŸ“‰ P7D é€±ç’°æ¯”è¡°é€€ (æœ¬é€± vs ä¸Šé€±)")
                if not alerts_weekly.empty:
                    st.dataframe(alerts_weekly, hide_index=True, use_container_width=True)
                else:
                    st.info("æœ¬é€±ç„¡é¡¯è‘—è¡°é€€é …ç›® (CPAèˆ‡CTRçš†ç©©å®š)")

            st.divider()

            # 30æ—¥æ¦‚æ³
            total_spend = df_p30d['èŠ±è²»é‡‘é¡ (TWD)'].sum()
            total_conv = df_p30d[conversion_col].sum()
            cpa_30d = total_spend / total_conv if total_conv > 0 else 0
            
            c1, c2, c3 = st.columns(3)
            c1.metric("è¿‘30æ—¥ç¸½èŠ±è²»", f"${total_spend:,.0f}")
            c2.metric(f"è¿‘30æ—¥ç¸½è½‰æ›", f"{total_conv:,.0f}")
            c3.metric("è¿‘30æ—¥å¹³å‡ CPA", f"${cpa_30d:,.0f}")
            
            # è¶¨å‹¢åœ–
            daily = df_p30d.groupby('å¤©æ•¸')[['èŠ±è²»é‡‘é¡ (TWD)', conversion_col, 'é€£çµé»æ“Šæ¬¡æ•¸', 'æ›å…‰æ¬¡æ•¸']].sum().reset_index()
            daily['æ—¥æœŸstr'] = daily['å¤©æ•¸'].dt.strftime('%m-%d')
            
            fig, ax1 = plt.subplots(figsize=(12, 5))
            ax2 = ax1.twinx()
            
            ax1.bar(daily['æ—¥æœŸstr'], daily['èŠ±è²»é‡‘é¡ (TWD)'], color='#ddd', label='èŠ±è²»', alpha=0.6)
            ax2.plot(daily['æ—¥æœŸstr'], daily[conversion_col], color='red', marker='o', label='è½‰æ›æ•¸', linewidth=2)
            
            ax1.set_xlabel('æ—¥æœŸ', fontproperties=font_prop)
            ax1.set_ylabel('èŠ±è²» (TWD)', fontproperties=font_prop)
            ax2.set_ylabel('è½‰æ›æ•¸', fontproperties=font_prop)
            if font_prop:
                for label in ax1.get_xticklabels(): label.set_fontproperties(font_prop)
            
            st.pyplot(fig)

        with tab2:
            st.markdown("### å„å€é–“è©³ç´°æ•¸æ“š")
            t_p1, t_p7, t_pp7, t_p30 = st.tabs(["P1D (æ˜¨æ—¥)", "P7D (æœ¬é€±)", "PP7D (ä¸Šé€±)", "P30D (æœˆå ±)"])
            
            # æº–å‚™å®Œæ•´æ•¸æ“š (å« Ad/AdSet)
            res_p1 = collect_period_results(df_p1d, 'P1D', conversion_col)
            res_p7 = collect_period_results(df_p7d, 'P7D', conversion_col)
            res_pp7 = collect_period_results(df_pp7d, 'PP7D', conversion_col)
            res_p30 = collect_period_results(df_p30d, 'P30D', conversion_col)
            
            def render_data_tab(results_list, unique_key):
                # results_list çµæ§‹: [0]Ad, [1]AdSet, [2]Campaign, [3]Detail
                view_mode = st.radio(
                    "é¸æ“‡æª¢è¦–å±¤ç´š:", 
                    ["è¡ŒéŠ·æ´»å‹• (Campaign)", "å»£å‘Šçµ„åˆ (AdSet)", "å»£å‘Š (Ad)", "è©³ç´°å±¤ç´š (AdSet + Ad)"],
                    horizontal=True,
                    key=unique_key
                )
                
                if view_mode == "è¡ŒéŠ·æ´»å‹• (Campaign)":
                    st.dataframe(results_list[2][1], use_container_width=True)
                elif view_mode == "å»£å‘Šçµ„åˆ (AdSet)":
                    st.dataframe(results_list[1][1], use_container_width=True)
                elif view_mode == "å»£å‘Š (Ad)":
                    st.dataframe(results_list[0][1], use_container_width=True)
                else:
                    st.dataframe(results_list[3][1], use_container_width=True)

            with t_p1: render_data_tab(res_p1, "radio_p1")
            with t_p7: render_data_tab(res_p7, "radio_p7")
            with t_pp7: render_data_tab(res_pp7, "radio_pp7")
            with t_p30: render_data_tab(res_p30, "radio_p30")

        # ä¸‹è¼‰å€
        with st.sidebar:
            st.divider()
            excel_stack = []
            excel_stack.append(('Trend_Daily', get_trend_data_excel(df_p30d, conversion_col)))
            excel_stack.extend(res_p1)
            excel_stack.extend(res_p7)
            excel_stack.extend(res_pp7)
            excel_stack.extend(res_p30)
            
            excel_bytes = to_excel_single_sheet_stacked(excel_stack, AI_CONSULTANT_PROMPT)
            
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ AI å®Œæ•´åˆ†æå ±è¡¨",
                data=excel_bytes,
                file_name=f"Full_Report_{max_date.strftime('%Y%m%d')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
            )

    except Exception as e:
        st.error(f"ç³»çµ±ç™¼ç”Ÿæœªé æœŸçš„éŒ¯èª¤: {e}")
        st.write("å»ºè­°æª¢æŸ¥ï¼š1. CSVæ ¼å¼æ˜¯å¦æ­£ç¢º 2. æ˜¯å¦åŒ…å«è½‰æ›/èŠ±è²»æ¬„ä½")
