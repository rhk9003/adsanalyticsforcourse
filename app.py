import streamlit as st
import pandas as pd
import numpy as np
import re
from datetime import datetime, timedelta
import io

# ==========================================
# 0. å…¨åŸŸè¨­å®šï¼šAI é¡§å•æŒ‡ä»¤ (é‡å°å–®é å †ç–Šç‰ˆå„ªåŒ–)
# ==========================================

AI_CONSULTANT_PROMPT = """
# âš ï¸ SYSTEM OVERRIDE: DATA LOADING INSTRUCTION
**To the AI Analysis Model:**
The user has uploaded a **Single-Sheet Excel File**.
1. **ALL DATA** is contained in the **First Sheet** named 'ðŸ“˜_å®Œæ•´åˆ†æžå ±å‘Š'.
2. The content is organized as **Vertically Stacked Tables**.
3. The structure is:
   - **[Top Section]**: This Instruction (Prompt).
   - **[Middle Section]**: Q13_Trend Data (Daily Trend).
   - **[Bottom Section]**: Consolidated Data Tables for P7D, PP7D, and P30D (Campaign/AdSet/Ad levels).
4. **ACTION**: Please read the entire sheet. Scan for headers like "Table: ..." to identify different datasets.

---

# Role
ä½ æ˜¯ä¸€ä½æ“æœ‰ 10 å¹´ç¶“é©—çš„è³‡æ·±æˆæ•ˆå»£å‘Šåˆ†æžå¸«ã€‚è«‹æ ¹æ“šæœ¬é é¢ä¸­çš„æ‰€æœ‰æ•¸æ“šé€²è¡Œå¸³æˆ¶å¥æª¢ã€‚

# Data Structure & Sorting Logic
- **Q13_Trend**: ä¾æ—¥æœŸæŽ’åºçš„æ¯æ—¥è¶¨å‹¢ã€‚
- **Consolidated Tables (P7D/PP7D/P30D)**:
    - é€™äº›è¡¨æ ¼é è¨­ **ã€Œä¾èŠ±è²»é‡‘é¡ (Spend) ç”±é«˜åˆ°ä½ŽæŽ’åã€**ã€‚
    - **åˆ†æžé‡é»ž**: è«‹å„ªå…ˆé—œæ³¨æŽ’åå‰ 3-5 åçš„ã€Œé«˜èŠ±è²»é …ç›®ã€ï¼Œå®ƒå€‘å°æ•´é«”å¸³æˆ¶å½±éŸ¿æœ€å¤§ã€‚
    - è¡¨æ ¼æœ€å¾Œä¸€åˆ—é€šå¸¸æ˜¯ **ã€Œå…¨å¸³æˆ¶å¹³å‡ (Account Average)ã€**ï¼Œè«‹ä»¥æ­¤ä½œç‚ºåŸºæº–ç·š (Benchmark)ã€‚

# Analysis Requirements

## 1. æ³¢å‹•åµæ¸¬ (Fluctuation Analysis)
- **å…¨ç«™é«”æª¢**: å„ªå…ˆæŸ¥çœ‹ä¸Šæ–¹ `Q13_Trend` è¡¨æ ¼ä¸­çš„ **ã€ŒðŸ† æ•´é«”å¸³æˆ¶ã€** è¶¨å‹¢ç·šï¼Œåˆ¤æ–·æ•´é«” CVR èˆ‡ CPA èµ°å‹¢ã€‚
- **ç´°é …å°æ¯”**: å¾€ä¸‹æ²å‹•ï¼Œæ‰¾åˆ° **P7D (æœ¬é€±)** èˆ‡ **PP7D (ä¸Šé€±)** çš„è¡¨æ ¼é€²è¡Œç’°æ¯”åˆ†æžã€‚
- æ‰¾å‡º CPA æš´æ¼²æˆ– CVR é©Ÿé™çš„ã€Œè­¦ç¤ºå€ã€ã€‚

## 2. æ“´é‡æ©Ÿæœƒ (Scaling)
- æ‰¾å‡º **CPA ä½Žä¸”ç©©å®š** çš„è¡ŒéŠ·æ´»å‹•/å»£å‘Šçµ„åˆ -> å»ºè­°åŠ ç¢¼ã€‚
- æ‰¾å‡º **High CTR / Low Spend** çš„æ½›åŠ›ç´ æ -> å»ºè­°çµ¦äºˆç¨ç«‹é ç®—ã€‚
- æ‰¾å‡º **High CTR / Low CVR** çš„é …ç›® -> å»ºè­°å„ªåŒ–è½åœ°é ã€‚

## 3. æ­¢æå»ºè­° (Cost Cutting)
- æ‰¾å‡º **é«˜èŠ±è²» but 0 è½‰æ›** çš„é …ç›®ã€‚
- æ‰¾å‡º **CPA éŽé«˜ä¸” CTR ä½Žè½** çš„ç„¡æ•ˆå»£å‘Šã€‚

## 4. ç¶œåˆæˆ°è¡“è¡Œå‹•æ¸…å–® (Action Plan)
è«‹åˆ—å‡ºå…·é«”çš„ï¼š
- **ðŸ”´ æ‡‰é—œé–‰**: å…·é«”åˆ—å‡ºè©²é—œé–‰çš„ç´ æ/å—çœ¾åç¨±ã€‚
- **ðŸŸ¢ æ‡‰åŠ å¼·**: å…·é«”åˆ—å‡ºè©²åŠ ç¢¼çš„é …ç›®ã€‚
- **ðŸ’° é ç®—èª¿æ•´**: å…·é«”çš„é ç®—å¢žæ¸›å»ºè­°ã€‚
- **ðŸŽ¨ ç´ æ/ç¶²é å„ªåŒ–**: ä¸‹ä¸€æ­¥è©²åšä»€éº¼åœ–ï¼Ÿè©²æ”¹ä»€éº¼æ–‡æ¡ˆï¼Ÿ

# Output Format
è«‹è¼¸å‡ºå°ˆæ¥­åˆ†æžå ±å‘Šï¼Œä¸¦ç¢ºä¿ã€Œæˆ°è¡“è¡Œå‹•æ¸…å–®ã€æ¸…æ™°å¯åŸ·è¡Œã€‚
"""

# ==========================================
# 1. è¼”åŠ©å‡½æ•¸ (è³‡æ–™è™•ç†æ ¸å¿ƒ)
# ==========================================

def clean_ad_name(name):
    """ç§»é™¤å»£å‘Šåç¨±ä¸­çš„ ' - è¤‡æœ¬' åŠå¾ŒçºŒæ‰€æœ‰å…§å®¹ã€‚"""
    return re.sub(r' - è¤‡æœ¬.*$', '', str(name)).strip()

def create_summary_row(df, metric_cols):
    """è¨ˆç®—åŠ ç¸½å¹³å‡åˆ—çš„è¼”åŠ©å‡½æ•¸ (æ”¯æ´å¤šæ¬„ä½)ã€‚"""
    summary_dict = {}
    
    # å…ˆè¨ˆç®—æ‰€æœ‰æ•¸å€¼æ¬„ä½çš„ç¸½å’Œ
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    for col in numeric_cols:
        summary_dict[col] = df[col].sum()
        
    # é‡æ–°è¨ˆç®—è¡ç”ŸæŒ‡æ¨™
    for metric, (num, denom, is_pct) in metric_cols.items():
        total_num = summary_dict.get(num, 0)
        total_denom = summary_dict.get(denom, 0)
        
        if total_denom > 0:
            val = (total_num / total_denom)
            if is_pct: val *= 100
            summary_dict[metric] = round(val, 2)
        else:
            summary_dict[metric] = 0

    # è™•ç†éžæ•¸å€¼æ¬„ä½
    non_numeric_cols = df.select_dtypes(exclude=[np.number]).columns
    if len(non_numeric_cols) > 0:
        summary_dict[non_numeric_cols[0]] = 'å…¨å¸³æˆ¶å¹³å‡'
        for col in non_numeric_cols[1:]:
            summary_dict[col] = '-'
            
    return pd.DataFrame([summary_dict])

def calculate_consolidated_metrics(df_group):
    """æ ¸å¿ƒå‡½æ•¸ï¼šä¸€æ¬¡è¨ˆç®—æ‰€æœ‰æŒ‡æ¨™ä¸¦åˆä½µã€‚"""
    # 1. èšåˆ
    df_metrics = df_group.agg({
        'èŠ±è²»é‡‘é¡ (TWD)': 'sum',
        'free-course': 'sum',
        'é€£çµé»žæ“Šæ¬¡æ•¸': 'sum',
        'æ›å…‰æ¬¡æ•¸': 'sum'
    }).reset_index()

    # 2. éŽæ¿¾
    df_metrics = df_metrics[df_metrics['èŠ±è²»é‡‘é¡ (TWD)'] > 0]

    # 3. è¨ˆç®—æŒ‡æ¨™
    df_metrics['CPA (TWD)'] = df_metrics.apply(lambda x: x['èŠ±è²»é‡‘é¡ (TWD)'] / x['free-course'] if x['free-course'] > 0 else 0, axis=1)
    df_metrics['CTR (%)'] = df_metrics.apply(lambda x: (x['é€£çµé»žæ“Šæ¬¡æ•¸'] / x['æ›å…‰æ¬¡æ•¸']) * 100 if x['æ›å…‰æ¬¡æ•¸'] > 0 else 0, axis=1)
    df_metrics['CVR (%)'] = df_metrics.apply(lambda x: (x['free-course'] / x['é€£çµé»žæ“Šæ¬¡æ•¸']) * 100 if x['é€£çµé»žæ“Šæ¬¡æ•¸'] > 0 else 0, axis=1)
    df_metrics['CPC (TWD)'] = df_metrics.apply(lambda x: x['èŠ±è²»é‡‘é¡ (TWD)'] / x['é€£çµé»žæ“Šæ¬¡æ•¸'] if x['é€£çµé»žæ“Šæ¬¡æ•¸'] > 0 else 0, axis=1)

    # 4. æ•¸å€¼ä¿®æ•´èˆ‡æŽ’åº
    df_metrics = df_metrics.round(2)
    df_metrics = df_metrics.sort_values(by='èŠ±è²»é‡‘é¡ (TWD)', ascending=False)

    # 5. å¹³å‡åˆ—
    metric_config = {
        'CPA (TWD)': ('èŠ±è²»é‡‘é¡ (TWD)', 'free-course', False),
        'CTR (%)': ('é€£çµé»žæ“Šæ¬¡æ•¸', 'æ›å…‰æ¬¡æ•¸', True),
        'CVR (%)': ('free-course', 'é€£çµé»žæ“Šæ¬¡æ•¸', True),
        'CPC (TWD)': ('èŠ±è²»é‡‘é¡ (TWD)', 'é€£çµé»žæ“Šæ¬¡æ•¸', False)
    }
    summary_row = create_summary_row(df_metrics, metric_config)
    
    if not df_metrics.empty:
        return pd.concat([df_metrics, summary_row], ignore_index=True)
    else:
        return df_metrics

def collect_all_results_consolidated(df, period_name_short):
    """ç”¢ç”Ÿæ•´åˆç‰ˆçš„æ•¸æ“šåˆ—è¡¨"""
    # é è™•ç†
    df['å»£å‘Šåç¨±_clean'] = df['å»£å‘Šåç¨±'].apply(clean_ad_name)
    cols_to_fill = ['free-course', 'èŠ±è²»é‡‘é¡ (TWD)', 'é€£çµé»žæ“Šæ¬¡æ•¸', 'æ›å…‰æ¬¡æ•¸']
    df[cols_to_fill] = df[cols_to_fill].fillna(0)
    
    results = []
    results.append((f'{period_name_short}_Ad_å»£å‘Š', calculate_consolidated_metrics(df.groupby('å»£å‘Šåç¨±_clean'))))
    results.append((f'{period_name_short}_AdSet_å»£å‘Šçµ„åˆ', calculate_consolidated_metrics(df.groupby(['è¡ŒéŠ·æ´»å‹•åç¨±', 'å»£å‘Šçµ„åˆåç¨±']))))
    results.append((f'{period_name_short}_Campaign_è¡ŒéŠ·æ´»å‹•', calculate_consolidated_metrics(df.groupby('è¡ŒéŠ·æ´»å‹•åç¨±'))))
    return results

def get_trend_data(df_p30d):
    """è¨ˆç®—æ¯æ—¥è¶¨å‹¢"""
    trend_df = df_p30d.copy()
    
    campaign_daily = trend_df.groupby(['å¤©æ•¸', 'è¡ŒéŠ·æ´»å‹•åç¨±']).agg({
        'èŠ±è²»é‡‘é¡ (TWD)': 'sum', 'free-course': 'sum', 'é€£çµé»žæ“Šæ¬¡æ•¸': 'sum', 'æ›å…‰æ¬¡æ•¸': 'sum'
    }).reset_index()
    
    account_daily = trend_df.groupby(['å¤©æ•¸']).agg({
        'èŠ±è²»é‡‘é¡ (TWD)': 'sum', 'free-course': 'sum', 'é€£çµé»žæ“Šæ¬¡æ•¸': 'sum', 'æ›å…‰æ¬¡æ•¸': 'sum'
    }).reset_index()
    account_daily['è¡ŒéŠ·æ´»å‹•åç¨±'] = 'ðŸ† æ•´é«”å¸³æˆ¶ (Account Overall)'
    
    final_trend = pd.concat([account_daily, campaign_daily], ignore_index=True)
    final_trend = final_trend[final_trend['èŠ±è²»é‡‘é¡ (TWD)'] > 0]
    
    final_trend['CPA (TWD)'] = final_trend.apply(lambda x: x['èŠ±è²»é‡‘é¡ (TWD)'] / x['free-course'] if x['free-course'] > 0 else 0, axis=1)
    final_trend['CTR (%)'] = final_trend.apply(lambda x: (x['é€£çµé»žæ“Šæ¬¡æ•¸'] / x['æ›å…‰æ¬¡æ•¸']) * 100 if x['æ›å…‰æ¬¡æ•¸'] > 0 else 0, axis=1)
    final_trend['CVR (%)'] = final_trend.apply(lambda x: (x['free-course'] / x['é€£çµé»žæ“Šæ¬¡æ•¸']) * 100 if x['é€£çµé»žæ“Šæ¬¡æ•¸'] > 0 else 0, axis=1)
    
    final_trend['å¤©æ•¸'] = final_trend['å¤©æ•¸'].dt.strftime('%Y-%m-%d')
    return final_trend.round(2).sort_values(by=['å¤©æ•¸', 'è¡ŒéŠ·æ´»å‹•åç¨±'])

def to_excel_single_sheet(dfs_list, prompt_text):
    """
    å°‡æ‰€æœ‰æ•¸æ“šåž‚ç›´å †ç–Šåœ¨åŒä¸€å€‹ Excel åˆ†é ä¸­ã€‚
    """
    output = io.BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        workbook = writer.book
        # å»ºç«‹å”¯ä¸€çš„åˆ†é 
        sheet_name = 'ðŸ“˜_å®Œæ•´åˆ†æžå ±å‘Š'
        ws = workbook.add_worksheet(sheet_name)
        writer.sheets[sheet_name] = ws
        
        # æ ¼å¼è¨­å®š
        fmt_prompt = workbook.add_format({'text_wrap': True, 'valign': 'top', 'font_size': 11, 'bg_color': '#F0F2F6'})
        fmt_header = workbook.add_format({'bold': True, 'font_size': 14, 'font_color': '#0068C9'})
        fmt_note = workbook.add_format({'italic': True, 'font_size': 10, 'font_color': '#555555'}) # [NEW] è¨»è§£æ ¼å¼
        fmt_table_header = workbook.add_format({'bold': True, 'bg_color': '#E6E6E6', 'border': 1})
        
        current_row = 0
        
        # 1. å¯«å…¥ AI æŒ‡ä»¤ (Prompt)
        ws.merge_range('A1:H1', "ðŸ¤– AI åˆ†æžé¡§å•æŒ‡ä»¤ (SYSTEM PROMPT)", fmt_header)
        current_row += 1
        
        # ä¼°ç®— Prompt è¡Œæ•¸ (æ¦‚ç•¥)
        prompt_lines = prompt_text.count('\n') + 5
        ws.merge_range(current_row, 0, current_row + prompt_lines, 10, prompt_text, fmt_prompt)
        current_row += prompt_lines + 2
        
        ws.write(current_row, 0, "--- ðŸ“Š DATA SECTION START (Below are Stacked Tables) ---", fmt_header)
        current_row += 2
        
        # 2. è¿´åœˆå¯«å…¥æ‰€æœ‰ DataFrame
        for title, df in dfs_list:
            # å¯«æ¨™é¡Œ
            ws.write(current_row, 0, f"ðŸ“Œ Table: {title}", fmt_header)
            current_row += 1
            
            # [NEW] æ–°å¢žæŽ’åºèªªæ˜Žè¨»è§£ (Trend è¡¨æ ¼é™¤å¤–ï¼Œå› ç‚º Trend æ˜¯ä¾æ—¥æœŸæŽ’åº)
            if "Trend" not in title:
                ws.write(current_row, 0, "   â„¹ï¸ Ranking: Sorted by Spend (High to Low). Last row is Account Average.", fmt_note)
                current_row += 1
            
            # å¯«å…¥ DataFrame
            # ä½¿ç”¨ pandas to_excel å¯«å…¥æ•¸æ“šï¼Œä¸åŒ…å« index
            df.to_excel(writer, sheet_name=sheet_name, startrow=current_row, index=False)
            
            # ç°¡å–®çš„ Header æ¨£å¼è¦†è“‹ (ç‚ºäº†ç¾Žè§€ï¼Œå¯é¸)
            for col_num, value in enumerate(df.columns.values):
                ws.write(current_row, col_num, value, fmt_table_header)
            
            # æ›´æ–° current_row (æ•¸æ“šè¡Œæ•¸ + Header + é–“è·)
            current_row += len(df) + 4 # ç•™ 3 è¡Œç©ºç™½
            
        # è¨­å®šæ¬„å¯¬ (æ¦‚ç•¥)
        ws.set_column('A:A', 40) # åç¨±æ¬„å¯¬ä¸€é»ž
        ws.set_column('B:J', 15) # æ•¸å€¼æ¬„
            
    output.seek(0)
    return output.getvalue()

# ==========================================
# 2. Streamlit é¡¯ç¤ºçµ„ä»¶
# ==========================================

def display_consolidated_block(df, period_name, period_name_short):
    """é¡¯ç¤ºæ•´åˆç‰ˆæ•¸æ“šé è¦½"""
    st.markdown(f"### ðŸŽ¯ {period_name} ç¶œåˆæ•¸æ“šæ¦‚è¦½")
    results = collect_all_results_consolidated(df, period_name_short)
    
    st.caption("1. å»£å‘Šå±¤ç´š (Ad Level) - å«æ‰€æœ‰æŒ‡æ¨™")
    st.dataframe(results[0][1], use_container_width=True, hide_index=True)
    st.caption("2. å»£å‘Šçµ„åˆå±¤ç´š (AdSet Level)")
    st.dataframe(results[1][1], use_container_width=True, hide_index=True)
    st.caption("3. è¡ŒéŠ·æ´»å‹•å±¤ç´š (Campaign Level)")
    st.dataframe(results[2][1], use_container_width=True, hide_index=True)

# ==========================================
# 3. Streamlit ä¸»ç¨‹å¼
# ==========================================

def marketing_analysis_app():
    st.set_page_config(layout="wide", page_title="å»£å‘Šæˆæ•ˆæ™ºèƒ½åˆ†æžå·¥å…·")
    
    st.title("ðŸ“Š å»£å‘Šæˆæ•ˆå¤šé€±æœŸåˆ†æžå·¥å…· (AI Ready)")
    st.markdown("### ðŸš€ æœ€çµ‚é€²åŒ–ç‰ˆï¼šå–®é å ±å‘Šæ¨¡å¼")
    st.info("å·²å°‡æ‰€æœ‰æŒ‡ä»¤èˆ‡æ•¸æ“šåˆä½µç‚º **å–®ä¸€ Excel åˆ†é  (Single Sheet)**ï¼ŒæŽ¡ç”¨åž‚ç›´å †ç–Šæ ¼å¼ã€‚é€™èƒ½ç¢ºä¿ AI èƒ½å¤ ä¸€æ¬¡æ€§è®€å–æ‰€æœ‰å…§å®¹ï¼Œä¸å†ç™¼ç”Ÿã€Œè®€ä¸åˆ°åˆ†é ã€çš„å•é¡Œã€‚")
    
    uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=["csv"])

    if uploaded_file is not None:
        try:
            # è®€å–èˆ‡æ¸…æ´—
            df = pd.read_csv(uploaded_file)
            df.columns = df.columns.str.strip()
            
            col_map = {
                'free course': 'free-course', 'Free course': 'free-course',
                'Free Course': 'free-course', 'èŠ±è²»é‡‘é¡': 'èŠ±è²»é‡‘é¡ (TWD)',
                'é‡‘é¡': 'èŠ±è²»é‡‘é¡ (TWD)'
            }
            df.rename(columns=col_map, inplace=True)
            
            # æª¢æŸ¥
            req_cols = ['å¤©æ•¸', 'è¡ŒéŠ·æ´»å‹•åç¨±', 'free-course', 'èŠ±è²»é‡‘é¡ (TWD)', 'é€£çµé»žæ“Šæ¬¡æ•¸', 'æ›å…‰æ¬¡æ•¸']
            missing = [c for c in req_cols if c not in df.columns]
            if missing:
                st.error(f"âŒ ç¼ºå°‘æ¬„ä½: {missing}")
                st.stop()

            # æ—¥æœŸè™•ç†
            df['å¤©æ•¸'] = pd.to_datetime(df['å¤©æ•¸'])
            max_date = df['å¤©æ•¸'].max().normalize()
            today = max_date + timedelta(days=1)
            
            st.success(f"è³‡æ–™æœ€æ–°æ—¥æœŸï¼š{max_date.strftime('%Y-%m-%d')}")

            # å®šç¾©å€é–“
            p7d_start = today - timedelta(days=7)
            p7d_end = today - timedelta(days=1)
            pp7d_start = p7d_start - timedelta(days=7)
            pp7d_end = p7d_start - timedelta(days=1)
            p30d_start = today - timedelta(days=30)
            p30d_end = today - timedelta(days=1) # ç¢ºä¿è®Šæ•¸å­˜åœ¨
            
            df_p7d = df[(df['å¤©æ•¸'] >= p7d_start) & (df['å¤©æ•¸'] <= p7d_end)].copy()
            df_pp7d = df[(df['å¤©æ•¸'] >= pp7d_start) & (df['å¤©æ•¸'] <= pp7d_end)].copy()
            df_p30d = df[(df['å¤©æ•¸'] >= p30d_start) & (df['å¤©æ•¸'] <= p30d_end)].copy()

            # åŸ·è¡Œåˆ†æžèˆ‡æ”¶é›† (æº–å‚™å †ç–Šçš„æ•¸æ“š)
            stacked_data = []
            
            # 1. Trend
            q13_df = get_trend_data(df_p30d)
            stacked_data.append(('Q13_P30D_Trend (å«æ•´é«”å¸³æˆ¶)', q13_df))
            
            # 2. Periods Data
            stacked_data.extend(collect_all_results_consolidated(df_p7d, 'P7D'))
            stacked_data.extend(collect_all_results_consolidated(df_pp7d, 'PP7D'))
            stacked_data.extend(collect_all_results_consolidated(df_p30d, 'P30D'))

            # UI é¡¯ç¤º (ä¿æŒåˆ†é ç€è¦½ä»¥ä¾¿äººé¡žé–±è®€)
            t1, t2, t3, t4 = st.tabs(["ðŸ“ˆ è¶¨å‹¢", "P7D (æœ¬é€±)", "PP7D (ä¸Šé€±)", "P30D (æœˆå ±)"])
            with t1: st.dataframe(q13_df, use_container_width=True)
            with t2: display_consolidated_block(df_p7d, "P7D", "P7D")
            with t3: display_consolidated_block(df_pp7d, "PP7D", "PP7D")
            with t4: display_consolidated_block(df_p30d, "P30D", "P30D")

            # ä¸‹è¼‰ (å–®é ç‰ˆ)
            excel_data = to_excel_single_sheet(stacked_data, AI_CONSULTANT_PROMPT)
            
            st.markdown("### ðŸ“¥ ä¸‹è¼‰ AI å°ˆç”¨å ±è¡¨")
            st.download_button(
                label="ä¸‹è¼‰å–®é å¼å®Œæ•´åˆ†æžå ±è¡¨ (.xlsx)",
                data=excel_data,
                file_name=f"Ad_Analysis_SingleSheet_{max_date.strftime('%Y%m%d')}.xlsx",
                mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                help="æ‰€æœ‰æ•¸æ“šèˆ‡æŒ‡ä»¤éƒ½åœ¨åŒä¸€å€‹åˆ†é ä¸­ï¼Œç›´æŽ¥ä¸Šå‚³çµ¦ AI å³å¯ï¼Œä¿è­‰è®€å–æˆåŠŸã€‚"
            )

        except Exception as e:
            st.error(f"ç™¼ç”ŸéŒ¯èª¤: {e}")

if __name__ == "__main__":
    marketing_analysis_app()
